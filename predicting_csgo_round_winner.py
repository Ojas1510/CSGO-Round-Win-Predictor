# -*- coding: utf-8 -*-
"""Predicting CSGO Round Winner

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tIsdRsSsYNGjydHrymOBgcAAK82r8GwA

# Predicting CSGO Round Winner
"""

#Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Importing  CSGO-Round-Winner-Classification dataset from Openml
import requests

url = "https://www.openml.org/data/download/22102255/dataset"
r = requests.get(url, allow_redirects=True)

"""# Data Preprocessing"""

with open("dataset.txt", "wb") as f:
    f.write(r.content)

# Data Extraction and cleaning
data = []

with open("dataset.txt", "r") as f:
    for line in f.read().split("\n"):
        if line.startswith("@") or line.startswith("%") or line == "":
            continue
        data.append(line)

data[:10]

columns = []

with open("dataset.txt", "r") as f:
    for line in f.read().split("\n"):
        if line.startswith("@ATTRIBUTE"):
            columns.append(line.split(" ")[1])

columns

with open("df.csv", "w") as f:
    f.write(",".join(columns))
    f.write("\n")
    f.write("\n".join(data))
df = pd.read_csv("df.csv")

df.head()

df.describe()

df.columns = columns
#columns

# Encoding Round winner collum and t-win where 0 is ct side winning and 1 is t side winning
df["t_win"] = df.round_winner.astype("category").cat.codes

"""### **Feature Selection:** <br>
Finding corealtion of all the features present with the target variable ie. T side winning. <br>
Then analyzing the correlation values of different features and picking out features with have heighest  valuses of absolute correlation (either postive or negative) with the target variable. <br>
This done beacuse dataset have 97 features which are a little too much so we reduced total number of features to 25.
"""

correlations = df[columns+["t_win"]].corr()
print(correlations['t_win'].apply(abs).sort_values(ascending=False).iloc[:25])

# Setting up a correlation threshold as 0.15
selected_columns = []

for col in columns+['t_win']:
    try:
        if abs(correlations[col]['t_win']) > 0.15:
            selected_columns.append(col)
    except KeyError:
        pass

df_selected = df[selected_columns]

df_selected

print(len(df_selected.columns))

df_selected.info()

# Correlation Heatmap
print(correlations['bomb_planted']['t_win'])
plt.figure(figsize=(18, 12))
sns.heatmap(df_selected.corr().sort_values(by='t_win'), annot=True, cmap="YlGnBu")

# Analyzing if the dataset is skewed
df_selected.hist(figsize=(18, 12))
plt.savefig("histo.png")

"""Upon analyzing these bar charts we found that dataset is skewed , so we need to
normalize the data.
"""

# Splitting the dataset in test train and then normalizing the training data.
from sklearn.model_selection import train_test_split

X, y = df_selected.drop(["t_win"], axis=1), df_selected["t_win"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""## KNN Classifier"""

# Using KNN classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()

knn.fit(X_train_scaled, y_train)

knn.score(X_test_scaled, y_test)

"""The accuracy of model obatained from KNN is 75.9%, which means it can predict correctly which team would win 3/4 of time. <br>
Now, using hyperparameter tunning for obtaining better accuracy. <br>
Using Grid Search for tunning hyperparameters. <br>
Note: Can also used Randomized Search for faster operation.
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
param_grid = {
    "n_neighbors": list(range(5, 17, 2)),
    "weights": ["uniform", "distance"]
}

knn = KNeighborsClassifier(n_jobs=-1)

clf = GridSearchCV(knn, param_grid, n_jobs=-1, verbose=2, cv=3)
clf.fit(X_train_scaled, y_train)

clf.best_estimator_

clf.best_estimator_.score(X_test_scaled, y_test)
# Accuracy is increased

"""## Random Forest Classifier <br>
It combines ensemble learning methods with the decision tree framework to create multiple randomly drawn decision trees from the data, averaging the results to output a result that often times leads to strong predictions/classifications.

"""

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_jobs=-1)

forest.fit(X_train_scaled, y_train)

forest.score(X_test_scaled, y_test)
# Accuracy obatained from Random Forest Classifier is heigher that KNN

"""Using Randomized Search for hyperparameter tunning"""

from sklearn.model_selection import RandomizedSearchCV
param_dist = {
    "n_estimators": [100, 200, 300, 400],
    "max_features": ["auto", "sqrt", "log2"],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "bootstrap": [True, False]
}

rf = RandomForestClassifier(n_jobs=-1, random_state=42)

clf1 = RandomizedSearchCV(rf, param_distributions=param_dist, n_jobs=-1, n_iter=6, verbose=2, cv=3)
clf1.fit(X_train_scaled, y_train)

clf1.best_estimator_

clf1.best_estimator_.score(X_test_scaled, y_test)

"""## Neural Networks
They are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains. <br>
Here, a simple feed forward neural network with three dense layer having 400,200 and 200 neurons and then an output neuron saying yes or no.

"""

from tensorflow import keras

model = keras.models.Sequential()
model.add(keras.layers.Input(shape=(20,)))
model.add(keras.layers.Dense(400, activation="relu"))
model.add(keras.layers.Dense(200, activation="relu"))
model.add(keras.layers.Dense(200, activation="relu"))
model.add(keras.layers.Dense(1, activation="sigmoid"))

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)

X_train_scaled_train, X_valid, y_train_train, y_valid = train_test_split(X_train_scaled, y_train, test_size=0.20)


model.fit(X_train_scaled_train, y_train_train, epochs=100, callbacks=[early_stopping_cb], validation_data=(X_valid, y_valid))

model.evaluate(X_test_scaled, y_test)

"""This model had lowest accuracy score"""

